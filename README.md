# advanced-deeplearning-course

- Practice 1:

In this assignment ,two optimizer were used(adaptive moment estimation and Stochastic gradient descent). The SGD was slower than Adam because it depends on the learning rate and the LR was quite small in this  assignment as a result, the model took time to converge while for adam, it leverages the power of adaptive learning rates methods to find individual learning rates for each parameter, as a result the training has faster convergence.


Note :although  the momentum parameter with the SGD helps us to progress faster but is still slower than adam.

 also, two activation function were used (Leaky relu and tanh).
