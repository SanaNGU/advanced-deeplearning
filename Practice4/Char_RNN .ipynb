{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char_RNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X601Lp7m4JQY"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rjm3qWp4hEs",
        "outputId": "c309c33e-c612-4db0-eb02-d64915d9e7fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        " "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frEl5xRN4hHn"
      },
      "source": [
        "#!ls \"/content/gdrive/My Drive\""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ9Rl6A54hKh"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('/content/gdrive/My Drive/project/data/tinyshakespeare/input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uxbvi6QR4hM7",
        "outputId": "dce2189f-364e-4339-87c3-5c9ae74ea16e"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJT4Cis4hPN"
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WboKW9YO5L2C",
        "outputId": "d6474eff-709d-4340-8a56-d16c2ca90944"
      },
      "source": [
        "print(encoded[:100])\n",
        "print(len(encoded))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7 39 42 36 15 50 58 39 15 39  0 32 10  1 63  6 32 40 52 42 32 50 23 32\n",
            " 50 22 42 52 55 32 32 37 50  2 10 43 50 40 46 42 15 24 32 42 27 50 24 32\n",
            "  2 42 50 61 32 50 36 22 32  2 54  9 63 63  8 26 26  1 63 47 22 32  2 54\n",
            " 27 50 36 22 32  2 54  9 63 63  7 39 42 36 15 50 58 39 15 39  0 32 10  1\n",
            " 63 19 52 46]\n",
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_7hzuaJ5TwH"
      },
      "source": [
        "# Pre-processing the data\n",
        " our LSTM expects an input that is one-hot encoded meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRXskDyM5L4y"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTiT0jFaU3__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Tlz0zp5L7L",
        "outputId": "2596166e-169e-4080-e051-8cd49a11b3b4"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aagWYeoe5L9N"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBssaGKq5L_S"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvAFhvpw5MBK",
        "outputId": "1b2b143e-556c-4c68-f020-313fac7e2f69"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 7 39 42 36 15 50 58 39 15 39]\n",
            " [32 42 32 15 52 50 23 32 50  2]\n",
            " [60  2 37 50 55  2 46 36 32 42]\n",
            " [28 42 39 32 40 27 50 40  2 39]\n",
            " [61 27 50 64  2 55 54 44 50 58]\n",
            " [24 50 23 39 10 37 50 36  2 43]\n",
            " [15 52 50 60 32 27 50 23 24 52]\n",
            " [50 39 40 50 43 52 46 50 36 26]]\n",
            "\n",
            "y\n",
            " [[39 42 36 15 50 58 39 15 39  0]\n",
            " [42 32 15 52 50 23 32 50  2 42]\n",
            " [ 2 37 50 55  2 46 36 32 42 50]\n",
            " [42 39 32 40 27 50 40  2 39 42]\n",
            " [27 50 64  2 55 54 44 50 58 52]\n",
            " [50 23 39 10 37 50 36  2 43 50]\n",
            " [52 50 60 32 27 50 23 24 52 50]\n",
            " [39 40 50 43 52 46 50 36 26 32]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QSlcJWq9XMA",
        "outputId": "28dd5522-4a08-4c9c-d450-5bec0935d3ed"
      },
      "source": [
        "#check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ898qYB9di4"
      },
      "source": [
        "# Defining the network with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ou9naSe9XOh"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        #define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        #define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZpq2CFAAV7"
      },
      "source": [
        "Time to train\n",
        "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
        "\n",
        "Below we're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform backpropagation, as usual!\n",
        "\n",
        "A couple of details about training:\n",
        "\n",
        "Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new tuple variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
        "We use clip_grad_norm_ to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrYw9imn9XQy"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                      \"ppl {:4.2f}\".format(math.exp(np.mean(val_losses))))\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhvTdzXASLg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VACVC12k9XS6",
        "outputId": "e0da6a33-d541-4e05-f4a0-1d067257efa9"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugPjyEEp9XVH",
        "outputId": "fba32860-f0b8-49b8-bc53-6848b30346fa"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.3995... Val Loss: 3.4169 ppl 30.47\n",
            "Epoch: 1/20... Step: 20... Loss: 3.3168... Val Loss: 3.3561 ppl 28.68\n",
            "Epoch: 1/20... Step: 30... Loss: 3.3410... Val Loss: 3.3459 ppl 28.39\n",
            "Epoch: 1/20... Step: 40... Loss: 3.3548... Val Loss: 3.3400 ppl 28.22\n",
            "Epoch: 1/20... Step: 50... Loss: 3.3349... Val Loss: 3.3377 ppl 28.16\n",
            "Epoch: 1/20... Step: 60... Loss: 3.3258... Val Loss: 3.3315 ppl 27.98\n",
            "Epoch: 1/20... Step: 70... Loss: 3.3427... Val Loss: 3.3284 ppl 27.89\n",
            "Epoch: 2/20... Step: 80... Loss: 3.3071... Val Loss: 3.3165 ppl 27.56\n",
            "Epoch: 2/20... Step: 90... Loss: 3.2815... Val Loss: 3.2892 ppl 26.82\n",
            "Epoch: 2/20... Step: 100... Loss: 3.2307... Val Loss: 3.2243 ppl 25.14\n",
            "Epoch: 2/20... Step: 110... Loss: 3.1655... Val Loss: 3.1165 ppl 22.57\n",
            "Epoch: 2/20... Step: 120... Loss: 3.0985... Val Loss: 3.0732 ppl 21.61\n",
            "Epoch: 2/20... Step: 130... Loss: 2.9722... Val Loss: 2.9024 ppl 18.22\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9169... Val Loss: 2.8683 ppl 17.61\n",
            "Epoch: 2/20... Step: 150... Loss: 2.7858... Val Loss: 2.7341 ppl 15.40\n",
            "Epoch: 3/20... Step: 160... Loss: 2.6954... Val Loss: 2.6372 ppl 13.97\n",
            "Epoch: 3/20... Step: 170... Loss: 2.6389... Val Loss: 2.5502 ppl 12.81\n",
            "Epoch: 3/20... Step: 180... Loss: 2.5641... Val Loss: 2.4948 ppl 12.12\n",
            "Epoch: 3/20... Step: 190... Loss: 2.5282... Val Loss: 2.4475 ppl 11.56\n",
            "Epoch: 3/20... Step: 200... Loss: 2.4611... Val Loss: 2.4065 ppl 11.09\n",
            "Epoch: 3/20... Step: 210... Loss: 2.4368... Val Loss: 2.3805 ppl 10.81\n",
            "Epoch: 3/20... Step: 220... Loss: 2.3887... Val Loss: 2.3499 ppl 10.48\n",
            "Epoch: 3/20... Step: 230... Loss: 2.3618... Val Loss: 2.3225 ppl 10.20\n",
            "Epoch: 4/20... Step: 240... Loss: 2.3442... Val Loss: 2.2893 ppl 9.87\n",
            "Epoch: 4/20... Step: 250... Loss: 2.3250... Val Loss: 2.2677 ppl 9.66\n",
            "Epoch: 4/20... Step: 260... Loss: 2.2908... Val Loss: 2.2433 ppl 9.42\n",
            "Epoch: 4/20... Step: 270... Loss: 2.2802... Val Loss: 2.2198 ppl 9.21\n",
            "Epoch: 4/20... Step: 280... Loss: 2.2691... Val Loss: 2.2053 ppl 9.07\n",
            "Epoch: 4/20... Step: 290... Loss: 2.2437... Val Loss: 2.1785 ppl 8.83\n",
            "Epoch: 4/20... Step: 300... Loss: 2.1997... Val Loss: 2.1618 ppl 8.69\n",
            "Epoch: 4/20... Step: 310... Loss: 2.1639... Val Loss: 2.1422 ppl 8.52\n",
            "Epoch: 5/20... Step: 320... Loss: 2.1751... Val Loss: 2.1213 ppl 8.34\n",
            "Epoch: 5/20... Step: 330... Loss: 2.1667... Val Loss: 2.1088 ppl 8.24\n",
            "Epoch: 5/20... Step: 340... Loss: 2.1387... Val Loss: 2.0984 ppl 8.15\n",
            "Epoch: 5/20... Step: 350... Loss: 2.1196... Val Loss: 2.0814 ppl 8.02\n",
            "Epoch: 5/20... Step: 360... Loss: 2.0954... Val Loss: 2.0728 ppl 7.95\n",
            "Epoch: 5/20... Step: 370... Loss: 2.0715... Val Loss: 2.0567 ppl 7.82\n",
            "Epoch: 5/20... Step: 380... Loss: 2.0850... Val Loss: 2.0415 ppl 7.70\n",
            "Epoch: 5/20... Step: 390... Loss: 2.0710... Val Loss: 2.0315 ppl 7.63\n",
            "Epoch: 6/20... Step: 400... Loss: 2.0586... Val Loss: 2.0189 ppl 7.53\n",
            "Epoch: 6/20... Step: 410... Loss: 2.0189... Val Loss: 2.0099 ppl 7.46\n",
            "Epoch: 6/20... Step: 420... Loss: 1.9672... Val Loss: 1.9948 ppl 7.35\n",
            "Epoch: 6/20... Step: 430... Loss: 1.9832... Val Loss: 1.9957 ppl 7.36\n",
            "Epoch: 6/20... Step: 440... Loss: 1.9689... Val Loss: 1.9801 ppl 7.24\n",
            "Epoch: 6/20... Step: 450... Loss: 2.0008... Val Loss: 1.9711 ppl 7.18\n",
            "Epoch: 6/20... Step: 460... Loss: 1.9423... Val Loss: 1.9564 ppl 7.07\n",
            "Epoch: 7/20... Step: 470... Loss: 1.9824... Val Loss: 1.9418 ppl 6.97\n",
            "Epoch: 7/20... Step: 480... Loss: 1.9456... Val Loss: 1.9356 ppl 6.93\n",
            "Epoch: 7/20... Step: 490... Loss: 1.9349... Val Loss: 1.9322 ppl 6.90\n",
            "Epoch: 7/20... Step: 500... Loss: 1.9349... Val Loss: 1.9261 ppl 6.86\n",
            "Epoch: 7/20... Step: 510... Loss: 1.9218... Val Loss: 1.9161 ppl 6.79\n",
            "Epoch: 7/20... Step: 520... Loss: 1.9243... Val Loss: 1.9063 ppl 6.73\n",
            "Epoch: 7/20... Step: 530... Loss: 1.9078... Val Loss: 1.9011 ppl 6.69\n",
            "Epoch: 7/20... Step: 540... Loss: 1.9056... Val Loss: 1.8999 ppl 6.69\n",
            "Epoch: 8/20... Step: 550... Loss: 1.9044... Val Loss: 1.8859 ppl 6.59\n",
            "Epoch: 8/20... Step: 560... Loss: 1.8886... Val Loss: 1.8781 ppl 6.54\n",
            "Epoch: 8/20... Step: 570... Loss: 1.8561... Val Loss: 1.8734 ppl 6.51\n",
            "Epoch: 8/20... Step: 580... Loss: 1.8981... Val Loss: 1.8699 ppl 6.49\n",
            "Epoch: 8/20... Step: 590... Loss: 1.8600... Val Loss: 1.8615 ppl 6.43\n",
            "Epoch: 8/20... Step: 600... Loss: 1.8349... Val Loss: 1.8523 ppl 6.37\n",
            "Epoch: 8/20... Step: 610... Loss: 1.8199... Val Loss: 1.8542 ppl 6.39\n",
            "Epoch: 8/20... Step: 620... Loss: 1.8264... Val Loss: 1.8416 ppl 6.31\n",
            "Epoch: 9/20... Step: 630... Loss: 1.8432... Val Loss: 1.8352 ppl 6.27\n",
            "Epoch: 9/20... Step: 640... Loss: 1.8163... Val Loss: 1.8289 ppl 6.23\n",
            "Epoch: 9/20... Step: 650... Loss: 1.8112... Val Loss: 1.8227 ppl 6.19\n",
            "Epoch: 9/20... Step: 660... Loss: 1.7926... Val Loss: 1.8196 ppl 6.17\n",
            "Epoch: 9/20... Step: 670... Loss: 1.8018... Val Loss: 1.8109 ppl 6.12\n",
            "Epoch: 9/20... Step: 680... Loss: 1.7970... Val Loss: 1.8140 ppl 6.14\n",
            "Epoch: 9/20... Step: 690... Loss: 1.7735... Val Loss: 1.8065 ppl 6.09\n",
            "Epoch: 9/20... Step: 700... Loss: 1.7610... Val Loss: 1.7995 ppl 6.05\n",
            "Epoch: 10/20... Step: 710... Loss: 1.7894... Val Loss: 1.7856 ppl 5.96\n",
            "Epoch: 10/20... Step: 720... Loss: 1.7722... Val Loss: 1.7863 ppl 5.97\n",
            "Epoch: 10/20... Step: 730... Loss: 1.7417... Val Loss: 1.7826 ppl 5.95\n",
            "Epoch: 10/20... Step: 740... Loss: 1.7339... Val Loss: 1.7762 ppl 5.91\n",
            "Epoch: 10/20... Step: 750... Loss: 1.7431... Val Loss: 1.7772 ppl 5.91\n",
            "Epoch: 10/20... Step: 760... Loss: 1.7432... Val Loss: 1.7749 ppl 5.90\n",
            "Epoch: 10/20... Step: 770... Loss: 1.7247... Val Loss: 1.7677 ppl 5.86\n",
            "Epoch: 10/20... Step: 780... Loss: 1.7340... Val Loss: 1.7585 ppl 5.80\n",
            "Epoch: 11/20... Step: 790... Loss: 1.7482... Val Loss: 1.7494 ppl 5.75\n",
            "Epoch: 11/20... Step: 800... Loss: 1.7206... Val Loss: 1.7516 ppl 5.76\n",
            "Epoch: 11/20... Step: 810... Loss: 1.6632... Val Loss: 1.7476 ppl 5.74\n",
            "Epoch: 11/20... Step: 820... Loss: 1.6825... Val Loss: 1.7421 ppl 5.71\n",
            "Epoch: 11/20... Step: 830... Loss: 1.6853... Val Loss: 1.7377 ppl 5.68\n",
            "Epoch: 11/20... Step: 840... Loss: 1.7225... Val Loss: 1.7349 ppl 5.67\n",
            "Epoch: 11/20... Step: 850... Loss: 1.6948... Val Loss: 1.7303 ppl 5.64\n",
            "Epoch: 12/20... Step: 860... Loss: 1.7067... Val Loss: 1.7201 ppl 5.59\n",
            "Epoch: 12/20... Step: 870... Loss: 1.6686... Val Loss: 1.7164 ppl 5.56\n",
            "Epoch: 12/20... Step: 880... Loss: 1.6681... Val Loss: 1.7184 ppl 5.58\n",
            "Epoch: 12/20... Step: 890... Loss: 1.6718... Val Loss: 1.7136 ppl 5.55\n",
            "Epoch: 12/20... Step: 900... Loss: 1.6679... Val Loss: 1.7139 ppl 5.55\n",
            "Epoch: 12/20... Step: 910... Loss: 1.6837... Val Loss: 1.7074 ppl 5.51\n",
            "Epoch: 12/20... Step: 920... Loss: 1.6813... Val Loss: 1.7043 ppl 5.50\n",
            "Epoch: 12/20... Step: 930... Loss: 1.6852... Val Loss: 1.7001 ppl 5.47\n",
            "Epoch: 13/20... Step: 940... Loss: 1.6865... Val Loss: 1.6889 ppl 5.41\n",
            "Epoch: 13/20... Step: 950... Loss: 1.6608... Val Loss: 1.6922 ppl 5.43\n",
            "Epoch: 13/20... Step: 960... Loss: 1.6326... Val Loss: 1.6902 ppl 5.42\n",
            "Epoch: 13/20... Step: 970... Loss: 1.6755... Val Loss: 1.6870 ppl 5.40\n",
            "Epoch: 13/20... Step: 980... Loss: 1.6633... Val Loss: 1.6878 ppl 5.41\n",
            "Epoch: 13/20... Step: 990... Loss: 1.6380... Val Loss: 1.6838 ppl 5.39\n",
            "Epoch: 13/20... Step: 1000... Loss: 1.6381... Val Loss: 1.6792 ppl 5.36\n",
            "Epoch: 13/20... Step: 1010... Loss: 1.6370... Val Loss: 1.6727 ppl 5.33\n",
            "Epoch: 14/20... Step: 1020... Loss: 1.6410... Val Loss: 1.6638 ppl 5.28\n",
            "Epoch: 14/20... Step: 1030... Loss: 1.6152... Val Loss: 1.6687 ppl 5.31\n",
            "Epoch: 14/20... Step: 1040... Loss: 1.6339... Val Loss: 1.6701 ppl 5.31\n",
            "Epoch: 14/20... Step: 1050... Loss: 1.6074... Val Loss: 1.6655 ppl 5.29\n",
            "Epoch: 14/20... Step: 1060... Loss: 1.6249... Val Loss: 1.6624 ppl 5.27\n",
            "Epoch: 14/20... Step: 1070... Loss: 1.6248... Val Loss: 1.6574 ppl 5.25\n",
            "Epoch: 14/20... Step: 1080... Loss: 1.6082... Val Loss: 1.6534 ppl 5.22\n",
            "Epoch: 14/20... Step: 1090... Loss: 1.5910... Val Loss: 1.6515 ppl 5.21\n",
            "Epoch: 15/20... Step: 1100... Loss: 1.6351... Val Loss: 1.6441 ppl 5.18\n",
            "Epoch: 15/20... Step: 1110... Loss: 1.6058... Val Loss: 1.6486 ppl 5.20\n",
            "Epoch: 15/20... Step: 1120... Loss: 1.5801... Val Loss: 1.6459 ppl 5.19\n",
            "Epoch: 15/20... Step: 1130... Loss: 1.5659... Val Loss: 1.6438 ppl 5.17\n",
            "Epoch: 15/20... Step: 1140... Loss: 1.5978... Val Loss: 1.6407 ppl 5.16\n",
            "Epoch: 15/20... Step: 1150... Loss: 1.5886... Val Loss: 1.6380 ppl 5.14\n",
            "Epoch: 15/20... Step: 1160... Loss: 1.5720... Val Loss: 1.6355 ppl 5.13\n",
            "Epoch: 15/20... Step: 1170... Loss: 1.5928... Val Loss: 1.6261 ppl 5.08\n",
            "Epoch: 16/20... Step: 1180... Loss: 1.6008... Val Loss: 1.6273 ppl 5.09\n",
            "Epoch: 16/20... Step: 1190... Loss: 1.5799... Val Loss: 1.6258 ppl 5.08\n",
            "Epoch: 16/20... Step: 1200... Loss: 1.5194... Val Loss: 1.6257 ppl 5.08\n",
            "Epoch: 16/20... Step: 1210... Loss: 1.5357... Val Loss: 1.6210 ppl 5.06\n",
            "Epoch: 16/20... Step: 1220... Loss: 1.5374... Val Loss: 1.6219 ppl 5.06\n",
            "Epoch: 16/20... Step: 1230... Loss: 1.5903... Val Loss: 1.6188 ppl 5.05\n",
            "Epoch: 16/20... Step: 1240... Loss: 1.5623... Val Loss: 1.6148 ppl 5.03\n",
            "Epoch: 17/20... Step: 1250... Loss: 1.5701... Val Loss: 1.6090 ppl 5.00\n",
            "Epoch: 17/20... Step: 1260... Loss: 1.5540... Val Loss: 1.6122 ppl 5.01\n",
            "Epoch: 17/20... Step: 1270... Loss: 1.5353... Val Loss: 1.6134 ppl 5.02\n",
            "Epoch: 17/20... Step: 1280... Loss: 1.5401... Val Loss: 1.6085 ppl 5.00\n",
            "Epoch: 17/20... Step: 1290... Loss: 1.5441... Val Loss: 1.6063 ppl 4.98\n",
            "Epoch: 17/20... Step: 1300... Loss: 1.5524... Val Loss: 1.6082 ppl 4.99\n",
            "Epoch: 17/20... Step: 1310... Loss: 1.5666... Val Loss: 1.6039 ppl 4.97\n",
            "Epoch: 17/20... Step: 1320... Loss: 1.5634... Val Loss: 1.5986 ppl 4.95\n",
            "Epoch: 18/20... Step: 1330... Loss: 1.5746... Val Loss: 1.5930 ppl 4.92\n",
            "Epoch: 18/20... Step: 1340... Loss: 1.5480... Val Loss: 1.5924 ppl 4.92\n",
            "Epoch: 18/20... Step: 1350... Loss: 1.5180... Val Loss: 1.5938 ppl 4.92\n",
            "Epoch: 18/20... Step: 1360... Loss: 1.5573... Val Loss: 1.5925 ppl 4.92\n",
            "Epoch: 18/20... Step: 1370... Loss: 1.5376... Val Loss: 1.5938 ppl 4.92\n",
            "Epoch: 18/20... Step: 1380... Loss: 1.5244... Val Loss: 1.5923 ppl 4.92\n",
            "Epoch: 18/20... Step: 1390... Loss: 1.5199... Val Loss: 1.5905 ppl 4.91\n",
            "Epoch: 18/20... Step: 1400... Loss: 1.5270... Val Loss: 1.5838 ppl 4.87\n",
            "Epoch: 19/20... Step: 1410... Loss: 1.5328... Val Loss: 1.5779 ppl 4.84\n",
            "Epoch: 19/20... Step: 1420... Loss: 1.4994... Val Loss: 1.5822 ppl 4.87\n",
            "Epoch: 19/20... Step: 1430... Loss: 1.5157... Val Loss: 1.5802 ppl 4.86\n",
            "Epoch: 19/20... Step: 1440... Loss: 1.5051... Val Loss: 1.5812 ppl 4.86\n",
            "Epoch: 19/20... Step: 1450... Loss: 1.5202... Val Loss: 1.5806 ppl 4.86\n",
            "Epoch: 19/20... Step: 1460... Loss: 1.5183... Val Loss: 1.5769 ppl 4.84\n",
            "Epoch: 19/20... Step: 1470... Loss: 1.5051... Val Loss: 1.5755 ppl 4.83\n",
            "Epoch: 19/20... Step: 1480... Loss: 1.4914... Val Loss: 1.5710 ppl 4.81\n",
            "Epoch: 20/20... Step: 1490... Loss: 1.5388... Val Loss: 1.5656 ppl 4.79\n",
            "Epoch: 20/20... Step: 1500... Loss: 1.4967... Val Loss: 1.5693 ppl 4.80\n",
            "Epoch: 20/20... Step: 1510... Loss: 1.4932... Val Loss: 1.5722 ppl 4.82\n",
            "Epoch: 20/20... Step: 1520... Loss: 1.4707... Val Loss: 1.5685 ppl 4.80\n",
            "Epoch: 20/20... Step: 1530... Loss: 1.4970... Val Loss: 1.5677 ppl 4.80\n",
            "Epoch: 20/20... Step: 1540... Loss: 1.4865... Val Loss: 1.5690 ppl 4.80\n",
            "Epoch: 20/20... Step: 1550... Loss: 1.4823... Val Loss: 1.5605 ppl 4.76\n",
            "Epoch: 20/20... Step: 1560... Loss: 1.5008... Val Loss: 1.5579 ppl 4.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWwvcwPA9XXE"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhXtsBQ9XY4"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpq_kW4EBgv3",
        "outputId": "5a4400a2-2bd2-4b2a-9244-6bee36b882ff"
      },
      "source": [
        "print(sample(net, 100, prime='The ', top_k=5))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The well be this store are,\n",
            "Some stands as it, would the complinature true,\n",
            "And saves a shall, we see the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPaxoyboB_oN",
        "outputId": "2babac39-653d-4b6d-8af2-d232b452f605"
      },
      "source": [
        "print(sample(net, 100, prime='What is', top_k=5))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is this seat way\n",
            "To the son, thou dost ster that his shall\n",
            "That's supper is nice shouse that baried him\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRDZPP2hB_w3",
        "outputId": "7f4c0087-f8bc-4976-b261-e56d154fa077"
      },
      "source": [
        "print(sample(net, 100, prime='Shall I give', top_k=5))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shall I give ano meaten,.\n",
            "\n",
            "First Servent:\n",
            "Hath shows my lond to him; and therefore we his stone;\n",
            "And, to have no \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ter2Ma57CK7Z"
      },
      "source": [
        "#print(sample(net, 100, prime='g09Z', top_k=5))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwcMGY0cHuv-"
      },
      "source": [
        "https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnRESfjaHwBX"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}